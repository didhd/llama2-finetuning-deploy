{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c707569-4e13-4329-9d68-7c45626e6115",
   "metadata": {},
   "source": [
    "# LLama2 LLM (Large Language Model) fine-tuning on SageMaker\n",
    "\n",
    "---\n",
    "\n",
    "* í—ˆê¹…í˜ì´ìŠ¤ ì¸ì¦ ì •ë³´ ì„¤ì •: huggingface-cli login\n",
    "    * https://huggingface.co/join\n",
    "    * https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e37cb8-b4b2-4856-aab4-942e3ffade96",
   "metadata": {},
   "source": [
    "## Note\n",
    "ì´ ë…¸íŠ¸ë¶ì€ SageMaker ê¸°ë³¸ APIë¥¼ ì°¸ì¡°í•˜ë¯€ë¡œ, SageMaker Studio, SageMaker ë…¸íŠ¸ë¶ ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” AWS CLIê°€ ì„¤ì •ëœ ë¡œì»¬ ì‹œìŠ¤í…œì—ì„œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤. SageMaker Studio ë˜ëŠ” SageMaker ë…¸íŠ¸ë¶ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° PyTorch ê¸°ë°˜ ì»¤ë„ì„ ì„ íƒí•˜ì„¸ìš”.\n",
    "í›ˆë ¨(Training) job ìˆ˜í–‰ ì‹œ ìµœì†Œ ml.g5.2xlarge í›ˆë ¨ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê¶Œì¥í•˜ë©°, ë¶„ì‚° í›ˆë ¨ ìˆ˜í–‰ ì‹œì—ëŠ” ml.g5.12xlarge í›ˆë ¨ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤. ë§Œì•½ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©ì— ì œí•œì´ ê±¸ë ¤ ìˆë‹¤ë©´ Request a service quota increase for SageMaker resourcesë¥¼ ì°¸ì¡°í•˜ì—¬ ì¸ìŠ¤í„´ìŠ¤ ì œí•œì„ í•´ì œí•´ ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a3683-5a5d-4b83-b552-f087c1acdea7",
   "metadata": {},
   "source": [
    "## Quick intro: PEFT or Parameter Efficient Fine-tuning\n",
    "\n",
    "PEFT (Parameter Efficient Fine-tuning)ëŠ” Hugging Faceì—ì„œ ê°œë°œí•œ ìƒˆë¡œìš´ ì˜¤í”ˆ ì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸(PLMs)ì„ ëª¨ë“  ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ë¯¸ì„¸ ì¡°ì •í•˜ì§€ ì•Šê³ ë„ ë‹¤ì–‘í•œ í•˜ìœ„ ì‘ìš© í”„ë¡œê·¸ë¨ì— íš¨ìœ¨ì ìœ¼ë¡œ ì ìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa58257-5868-4fc9-8fb6-3b40a13b68af",
   "metadata": {},
   "source": [
    "QLoRA (Quantized Low-Rank Adapters)ëŠ” ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸ì„ 4ë¹„íŠ¸ë¡œ ì–‘ìí™”í•˜ê³ , ì†Œê·œëª¨ì˜ \"ì €ì°¨ì› ì–´ëŒ‘í„°\"ë¥¼ ë¶€ì°©í•˜ì—¬ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” íš¨ìœ¨ì ì¸ ë¯¸ì„¸ ì¡°ì • ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ë©´ ìµœëŒ€ 650ì–µ ê°œì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ ëª¨ë¸ì„ ë‹¨ì¼ GPUì—ì„œ ë¯¸ì„¸ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë†€ëê²Œë„, QLoRAëŠ” ì „ì²´ ì •ë°€ë„ ë¯¸ì„¸ ì¡°ì •ì˜ ì„±ëŠ¥ì„ ë§ì¶”ë©° ì–¸ì–´ ì‘ì—…ì— ìµœì‹  ê²°ê³¼ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "ìš°ë¦¬ì˜ ì˜ˆì‹œì—ì„œ, ìš°ë¦¬ëŠ” Hugging Face Transformers, Accelerate, ê·¸ë¦¬ê³  PEFTë¥¼ í™œìš©í•  ê²ƒì…ë‹ˆë‹¤. ì´ ì¡°í•©ì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨, ìš°ë¦¬ëŠ” ê°•ë ¥í•œ ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸ì„ ë” íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©ìì˜ íŠ¹ì • ìš”êµ¬ì— ë§ê²Œ ë¯¸ì„¸ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Hugging Faceì˜ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë‹¤ì–‘í•œ NLP ì‘ì—…ì— ì‚¬ìš©ë˜ëŠ” ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì œê³µí•˜ë©°, AccelerateëŠ” ëª¨ë¸ í›ˆë ¨ì„ ë” ë¹ ë¥´ê³  ì‰½ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤. PEFTëŠ” ì´ ëª¨ë“  ê²ƒì„ ê²°í•©í•˜ì—¬ ëª¨ë¸ì˜ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd65ff-3d03-4d08-b3e6-945b6f9345e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. ê°œë°œ í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd1aaa-8a11-412c-86fc-9e0c151e7d55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.31.0\" \"datasets[s3]==2.13.0\" sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefdd782-8a46-4214-9330-d23727a546e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Access LLaMA 2\n",
    "í›ˆë ¨ì„ ì‹œì‘í•˜ê¸° ì „ì—, ìš°ë¦¬ëŠ” llama 2ì˜ ë¼ì´ì„ ìŠ¤ë¥¼ ìˆ˜ë½í–ˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. ëª¨ë¸ í˜ì´ì§€ì—ì„œ 'ë™ì˜í•˜ê³  ì €ì¥ì†Œì— ì ‘ê·¼í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•¨ìœ¼ë¡œì¨ ë¼ì´ì„ ìŠ¤ë¥¼ ìˆ˜ë½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "* [LLaMa 7B](https://huggingface.co/meta-llama/Llama-2-7b-hf)\n",
    "\n",
    "LLaMA 2 ìì‚°ì— ì ‘ê·¼í•˜ë ¤ë©´ Hugging Face ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì—¬ ì´ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289230c0-dd71-4c4f-b275-eb7a148986b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HuggingFace Token ì •ë³´ë¥¼ ì•„ë˜ì— ì…ë ¥í•©ë‹ˆë‹¤.\n",
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec33763-9555-46b5-a0cd-f980de8dece6",
   "metadata": {},
   "source": [
    "ë§Œì•½ ë¡œì»¬ í™˜ê²½ì—ì„œ Sagemakerë¥¼ ì‚¬ìš©í•  ì˜ˆì •ì´ë¼ë©´, Sagemakerì— í•„ìš”í•œ ê¶Œí•œì´ ìˆëŠ” IAM ì—­í• ì— ëŒ€í•œ ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤. [ì—¬ê¸°](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)ì—ì„œ ë” ìì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c3a02-c095-4dc6-afc7-58832941eb3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cc753f-eef0-4631-8173-a8af0a8950b4",
   "metadata": {},
   "source": [
    "# 2. ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ê³  ì¤€ë¹„í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af914eec-b92c-41bd-ac6c-923b780a151a",
   "metadata": {
    "tags": []
   },
   "source": [
    "`kyujinpy/KOR-OpenOrca-Platypus` ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´, ğŸ¤— Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ load_dataset() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d640aff2-954b-4a57-918c-c56e1190092e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"kyujinpy/KOR-OpenOrca-Platypus\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f999b996-f5f0-4468-8f94-60b778cde62b",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì„ ì§€ì‹œì ìœ¼ë¡œ íŠœë‹í•˜ê¸° ìœ„í•´ì„œëŠ” êµ¬ì¡°í™”ëœ ì˜ˆì‹œë“¤ì„ ì§€ì‹œì‚¬í•­ì„ í†µí•´ ì„¤ëª…ëœ ì¼ë ¨ì˜ ì‘ì—…ë“¤ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤. ìƒ˜í”Œì„ ë°›ì•„ ìš°ë¦¬ì˜ í˜•ì‹ ì§€ì‹œë¬¸ìœ¼ë¡œ ë³€í™˜ëœ ë¬¸ìì—´ì„ ë°˜í™˜í•˜ëŠ” formatting_functionì„ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312f9ce0-1462-4979-80a2-acd157f6d0ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_dataset(sample):\n",
    "    instruction = f\"### Instruction:\\n{sample['instruction']}\"\n",
    "    context = f\"### Input:\\n{sample['input']}\" if len(sample[\"input\"]) > 0 else None\n",
    "    response = f\"### Response:\\n{sample['output']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7db9bcd-dfad-4d81-b242-5a2046b760dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "ìš°ë¦¬ì˜ formatting í•¨ìˆ˜ë¥¼ ì„ì˜ì˜ ì˜ˆì‹œì— ì ìš©í•´ í…ŒìŠ¤íŠ¸í•´ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084d570-a1a2-453e-bc83-77c9b74d3229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dataset(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e3c3d5-7dd3-4106-8690-94a51a20691f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)  # You can specify a seed for reproducibility\n",
    "dataset = dataset.select(range(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cb840a-bed5-461e-8671-a5de5cabf1ef",
   "metadata": {},
   "source": [
    "ìƒ˜í”Œë“¤ì„ í˜•ì‹í™”í•˜ëŠ” ê²ƒ ì™¸ì—ë„, ë” íš¨ìœ¨ì ì¸ í›ˆë ¨ì„ ìœ„í•´ ì—¬ëŸ¬ ìƒ˜í”Œë“¤ì„ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ ë¬¶ê³  ì‹¶ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900bdbd4-4852-46f1-8958-db3d832882c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\" # sharded weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19df0d60-b86a-449e-9ec5-fa9ece6b81f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "ì£¼ì–´ì§„ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ë¡œ ìƒ˜í”Œë“¤ì„ ë¬¶ê³  í† í°í™”í•˜ê¸° ìœ„í•´ ëª‡ ê°€ì§€ ë„ìš°ë¯¸ í•¨ìˆ˜ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aca9a7-8129-46d3-a488-b7b4db668770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dataset(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c0398-6417-45cc-bb13-56902e823612",
   "metadata": {},
   "source": [
    "ë°ì´í„°ì…‹ì„ ì²˜ë¦¬í•œ í›„ì—ëŠ” ìƒˆë¡œìš´ íŒŒì¼ ì‹œìŠ¤í…œ í†µí•© ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ S3ì— ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•  ì˜ˆì •ì…ë‹ˆë‹¤. sess.default_bucket()ì„ ì‚¬ìš©í•˜ê³  ìˆìœ¼ë‹ˆ, ë‹¤ë¥¸ S3 ë²„í‚·ì— ë°ì´í„°ì…‹ì„ ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´ ì´ë¥¼ ì¡°ì •í•˜ì„¸ìš”. ìš°ë¦¬ëŠ” í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë‚˜ì¤‘ì— S3 ê²½ë¡œë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355400d5-a5de-4d0c-8c80-51620fa6d2dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/llama/kordata/ko-en-llama2'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d3811f-793f-4508-9867-4a69a1b1e3c6",
   "metadata": {},
   "source": [
    "# 3. Amazon SageMakerì—ì„œ QLoRAë¥¼ ì‚¬ìš©í•˜ì—¬ LLaMA 7B ë¯¸ì„¸ ì¡°ì •í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75274c86-5752-4cec-989a-0621f7044ef4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## QLoRAë¥¼ ì‚¬ìš©í•œ LLaMA 7B ë¯¸ì„¸ ì¡°ì • (Amazon SageMaker)\n",
    "\n",
    "### ê°œìš”\n",
    "- **ë°©ë²•**: Tim Dettmers ë“±ì´ ì‘ì„±í•œ ë…¼ë¬¸ \"QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation\"ì—ì„œ ì†Œê°œëœ ìµœê·¼ ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- **QLoRA**: ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë™ì•ˆ ì¤„ì´ëŠ” ìƒˆë¡œìš´ ê¸°ìˆ ë¡œ, ì„±ëŠ¥ì„ í¬ìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "\n",
    "### QLoRA ì‘ë™ ë°©ì‹\n",
    "1. ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ 4ë¹„íŠ¸ë¡œ ì–‘ìí™”í•˜ê³  ê³ ì •í•©ë‹ˆë‹¤.\n",
    "2. ì‘ê³  í›ˆë ¨ ê°€ëŠ¥í•œ ì–´ëŒ‘í„° ë ˆì´ì–´(LoRA)ë¥¼ ë¶€ì°©í•©ë‹ˆë‹¤.\n",
    "3. ì–‘ìí™”ëœ ê³ ì • ëª¨ë¸ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•˜ë©´ì„œ ì–´ëŒ‘í„° ë ˆì´ì–´ë§Œ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "### êµ¬í˜„\n",
    "- **run_clm.py**: QLoRAë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” PEFTë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ëŠ” í›ˆë ¨ í›„ LoRA ê°€ì¤‘ì¹˜ë¥¼ ëª¨ë¸ ê°€ì¤‘ì¹˜ì— ë³‘í•©í•©ë‹ˆë‹¤. ì¶”ê°€ ì½”ë“œ ì—†ì´ ëª¨ë¸ì„ ì¼ë°˜ ëª¨ë¸ì²˜ëŸ¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- **ì¤‘ìš”**: SageMakerê°€ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ë ¤ë©´ source_dir í´ë”ì— requirements.txtë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "### HuggingFace Estimator (Amazon SageMaker í›ˆë ¨ ì‘ì—… ìƒì„±)\n",
    "- **ê¸°ëŠ¥**: Amazon SageMakerì˜ í›ˆë ¨ ë° ë°°í¬ ì‘ì—…ì„ ì¢…ë‹¨ ê°„ ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
    "- **ê´€ë¦¬**: í•„ìš”í•œ EC2 ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‹œì‘í•˜ê³  ê´€ë¦¬í•˜ë©°, ì˜¬ë°”ë¥¸ HuggingFace ì»¨í…Œì´ë„ˆë¥¼ ì œê³µí•˜ê³ , ì œê³µëœ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì—…ë¡œë“œí•˜ë©°, S3 ë²„í‚·ì—ì„œ ì»¨í…Œì´ë„ˆì˜ /opt/ml/input/dataë¡œ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "\n",
    "### í•˜ë“œì›¨ì–´ ìš”êµ¬ ì‚¬í•­\n",
    "- **ì‹¤í—˜**: ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸°ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì¸ìŠ¤í„´ìŠ¤ ìœ í˜•ì„ ê²°ì •í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.\n",
    "- **ê²°ê³¼**:\n",
    "\n",
    "| ëª¨ë¸       | ì¸ìŠ¤í„´ìŠ¤ ìœ í˜•     | ìµœëŒ€ ë°°ì¹˜ í¬ê¸° | ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ |\n",
    "|------------|----------------|--------------|--------------|\n",
    "| LLaMa 7B   | (ml.)g5.4xlarge | 3            | 2048         |\n",
    "| LLaMa 13B  | (ml.)g5.4xlarge | 2            | 2048         |\n",
    "| LLaMa 70B  | (ml.)p4d.24xlarge | 1++ (ì¶”ê°€ í…ŒìŠ¤íŠ¸ í•„ìš”) | 2048 |\n",
    "\n",
    "- **ì°¸ê³ **: g5.2xlarge ëŒ€ì‹  g5.4xlarge ì¸ìŠ¤í„´ìŠ¤ ìœ í˜•ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆì§€ë§Œ, LoRA ê°€ì¤‘ì¹˜ë¥¼ ëª¨ë¸ ê°€ì¤‘ì¹˜ì— ë³‘í•©í•˜ë ¤ë©´ ëª¨ë¸ì´ ë©”ëª¨ë¦¬ì— ë§ì•„ì•¼ í•˜ë¯€ë¡œ merge_weights ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•˜ê³  í›ˆë ¨ í›„ merge_adapter_weights.pyë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- **ì°¸ê³ 2**: Workshopì—ëŠ” í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ì¸ìŠ¤í„´ìŠ¤ì¸ p3.2xlarge ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f237cea3-7e20-433f-8484-87a0e4eaa5e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'epochs': 1,                                      # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                 # batch size for training\n",
    "  'lr': 1e-4,                                       # learning rate used during training\n",
    "  'hf_token': HfFolder.get_token(),                 # huggingface token to access llama 2\n",
    "  'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.p3.2xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "    disable_output_compression = True,         # not compress output to save training time and cost\n",
    "    max_run              = 432000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b80895-cf1e-4ff9-b626-9da75a43eece",
   "metadata": {},
   "source": [
    "ì´ì œ .fit() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ìš°ë¦¬ì˜ S3 ê²½ë¡œë¥¼ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ì— ì „ë‹¬í•¨ìœ¼ë¡œì¨ í›ˆë ¨ ì‘ì—…ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cfee6a-1379-431b-8b73-5e4c7b662e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad60cca1",
   "metadata": {},
   "source": [
    "ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ê¸° ì „ gradioë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d86835-7395-4d78-a90d-5361ab2bf9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio -q"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
