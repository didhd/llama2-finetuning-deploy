{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c707569-4e13-4329-9d68-7c45626e6115",
   "metadata": {},
   "source": [
    "# LLama2 LLM (Large Language Model) fine-tuning on SageMaker\n",
    "\n",
    "---\n",
    "\n",
    "* 허깅페이스 인증 정보 설정: huggingface-cli login\n",
    "    * https://huggingface.co/join\n",
    "    * https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e37cb8-b4b2-4856-aab4-942e3ffade96",
   "metadata": {},
   "source": [
    "## Note\n",
    "이 노트북은 SageMaker 기본 API를 참조하므로, SageMaker Studio, SageMaker 노트북 인스턴스 또는 AWS CLI가 설정된 로컬 시스템에서 실행해야 합니다. SageMaker Studio 또는 SageMaker 노트북 인스턴스를 사용하는 경우 PyTorch 기반 커널을 선택하세요.\n",
    "훈련(Training) job 수행 시 최소 ml.g5.2xlarge 훈련 인스턴스를 권장하며, 분산 훈련 수행 시에는 ml.g5.12xlarge 훈련 인스턴스를 권장합니다. 만약 인스턴스 사용에 제한이 걸려 있다면 Request a service quota increase for SageMaker resources를 참조하여 인스턴스 제한을 해제해 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a3683-5a5d-4b83-b552-f087c1acdea7",
   "metadata": {},
   "source": [
    "## Quick intro: PEFT or Parameter Efficient Fine-tuning\n",
    "\n",
    "PEFT (Parameter Efficient Fine-tuning)는 Hugging Face에서 개발한 새로운 오픈 소스 라이브러리로, 사전 훈련된 언어 모델(PLMs)을 모든 모델의 매개변수를 미세 조정하지 않고도 다양한 하위 응용 프로그램에 효율적으로 적용할 수 있게 해줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa58257-5868-4fc9-8fb6-3b40a13b68af",
   "metadata": {},
   "source": [
    "QLoRA (Quantized Low-Rank Adapters)는 사전 훈련된 언어 모델을 4비트로 양자화하고, 소규모의 \"저차원 어댑터\"를 부착하여 미세 조정하는 효율적인 미세 조정 기술입니다. 이 기술을 사용하면 최대 650억 개의 매개변수를 가진 모델을 단일 GPU에서 미세 조정할 수 있습니다. 놀랍게도, QLoRA는 전체 정밀도 미세 조정의 성능을 맞추며 언어 작업에 최신 결과를 달성합니다.\n",
    "\n",
    "우리의 예시에서, 우리는 Hugging Face Transformers, Accelerate, 그리고 PEFT를 활용할 것입니다. 이 조합을 사용함으로써, 우리는 강력한 사전 훈련된 언어 모델을 더 효율적으로 사용자의 특정 요구에 맞게 미세 조정할 수 있습니다. Hugging Face의 Transformers 라이브러리는 다양한 NLP 작업에 사용되는 사전 훈련된 모델을 제공하며, Accelerate는 모델 훈련을 더 빠르고 쉽게 만들어주는 도구입니다. PEFT는 이 모든 것을 결합하여 모델의 효율성과 성능을 극대화합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd65ff-3d03-4d08-b3e6-945b6f9345e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. 개발 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd1aaa-8a11-412c-86fc-9e0c151e7d55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.31.0\" \"datasets[s3]==2.13.0\" sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefdd782-8a46-4214-9330-d23727a546e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Access LLaMA 2\n",
    "훈련을 시작하기 전에, 우리는 llama 2의 라이선스를 수락했는지 확인해야 합니다. 모델 페이지에서 '동의하고 저장소에 접근하기' 버튼을 클릭함으로써 라이선스를 수락할 수 있습니다.\n",
    "\n",
    "* [LLaMa 7B](https://huggingface.co/meta-llama/Llama-2-7b-hf)\n",
    "\n",
    "LLaMA 2 자산에 접근하려면 Hugging Face 계정으로 로그인해야 합니다. 다음 명령을 실행하여 이를 수행할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289230c0-dd71-4c4f-b275-eb7a148986b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HuggingFace Token 정보를 아래에 입력합니다.\n",
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec33763-9555-46b5-a0cd-f980de8dece6",
   "metadata": {},
   "source": [
    "만약 로컬 환경에서 Sagemaker를 사용할 예정이라면, Sagemaker에 필요한 권한이 있는 IAM 역할에 대한 접근이 필요합니다. [여기](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)에서 더 자세한 정보를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c3a02-c095-4dc6-afc7-58832941eb3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cc753f-eef0-4631-8173-a8af0a8950b4",
   "metadata": {},
   "source": [
    "# 2. 데이터셋을 불러오고 준비하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af914eec-b92c-41bd-ac6c-923b780a151a",
   "metadata": {
    "tags": []
   },
   "source": [
    "`kyujinpy/KOR-OpenOrca-Platypus` 데이터셋을 불러오기 위해, 🤗 Datasets 라이브러리의 load_dataset() 메서드를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d640aff2-954b-4a57-918c-c56e1190092e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"kyujinpy/KOR-OpenOrca-Platypus\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f999b996-f5f0-4468-8f94-60b778cde62b",
   "metadata": {},
   "source": [
    "모델을 지시적으로 튜닝하기 위해서는 구조화된 예시들을 지시사항을 통해 설명된 일련의 작업들로 변환해야 합니다. 샘플을 받아 우리의 형식 지시문으로 변환된 문자열을 반환하는 formatting_function을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312f9ce0-1462-4979-80a2-acd157f6d0ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_dataset(sample):\n",
    "    instruction = f\"### Instruction:\\n{sample['instruction']}\"\n",
    "    context = f\"### Input:\\n{sample['input']}\" if len(sample[\"input\"]) > 0 else None\n",
    "    response = f\"### Response:\\n{sample['output']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7db9bcd-dfad-4d81-b242-5a2046b760dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "우리의 formatting 함수를 임의의 예시에 적용해 테스트해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084d570-a1a2-453e-bc83-77c9b74d3229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dataset(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e3c3d5-7dd3-4106-8690-94a51a20691f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42)  # You can specify a seed for reproducibility\n",
    "dataset = dataset.select(range(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cb840a-bed5-461e-8671-a5de5cabf1ef",
   "metadata": {},
   "source": [
    "샘플들을 형식화하는 것 외에도, 더 효율적인 훈련을 위해 여러 샘플들을 하나의 시퀀스로 묶고 싶습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900bdbd4-4852-46f1-8958-db3d832882c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\" # sharded weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19df0d60-b86a-449e-9ec5-fa9ece6b81f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "주어진 길이의 시퀀스로 샘플들을 묶고 토큰화하기 위해 몇 가지 도우미 함수들을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aca9a7-8129-46d3-a488-b7b4db668770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dataset(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c0398-6417-45cc-bb13-56902e823612",
   "metadata": {},
   "source": [
    "데이터셋을 처리한 후에는 새로운 파일 시스템 통합 기능을 사용하여 S3에 데이터셋을 업로드할 예정입니다. sess.default_bucket()을 사용하고 있으니, 다른 S3 버킷에 데이터셋을 저장하고 싶다면 이를 조정하세요. 우리는 훈련 스크립트에서 나중에 S3 경로를 사용할 것입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355400d5-a5de-4d0c-8c80-51620fa6d2dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/llama/kordata/ko-en-llama2'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d3811f-793f-4508-9867-4a69a1b1e3c6",
   "metadata": {},
   "source": [
    "# 3. Amazon SageMaker에서 QLoRA를 사용하여 LLaMA 7B 미세 조정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75274c86-5752-4cec-989a-0621f7044ef4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## QLoRA를 사용한 LLaMA 7B 미세 조정 (Amazon SageMaker)\n",
    "\n",
    "### 개요\n",
    "- **방법**: Tim Dettmers 등이 작성한 논문 \"QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation\"에서 소개된 최근 방법을 사용합니다.\n",
    "- **QLoRA**: 대규모 언어 모델의 메모리 사용량을 미세 조정하는 동안 줄이는 새로운 기술로, 성능을 희생하지 않습니다.\n",
    "\n",
    "### QLoRA 작동 방식\n",
    "1. 사전 훈련된 모델을 4비트로 양자화하고 고정합니다.\n",
    "2. 작고 훈련 가능한 어댑터 레이어(LoRA)를 부착합니다.\n",
    "3. 양자화된 고정 모델을 컨텍스트로 사용하면서 어댑터 레이어만 미세 조정합니다.\n",
    "\n",
    "### 구현\n",
    "- **run_clm.py**: QLoRA를 사용하여 모델을 훈련하는 PEFT를 구현합니다. 스크립트는 훈련 후 LoRA 가중치를 모델 가중치에 병합합니다. 추가 코드 없이 모델을 일반 모델처럼 사용할 수 있습니다.\n",
    "- **중요**: SageMaker가 필요한 라이브러리를 설치하려면 source_dir 폴더에 requirements.txt를 추가해야 합니다.\n",
    "\n",
    "### HuggingFace Estimator (Amazon SageMaker 훈련 작업 생성)\n",
    "- **기능**: Amazon SageMaker의 훈련 및 배포 작업을 종단 간 관리합니다.\n",
    "- **관리**: 필요한 EC2 인스턴스를 시작하고 관리하며, 올바른 HuggingFace 컨테이너를 제공하고, 제공된 스크립트를 업로드하며, S3 버킷에서 컨테이너의 /opt/ml/input/data로 데이터를 다운로드합니다.\n",
    "\n",
    "### 하드웨어 요구 사항\n",
    "- **실험**: 다양한 모델 크기에 사용할 수 있는 인스턴스 유형을 결정하기 위해 여러 실험을 진행했습니다.\n",
    "- **결과**:\n",
    "\n",
    "| 모델       | 인스턴스 유형     | 최대 배치 크기 | 컨텍스트 길이 |\n",
    "|------------|----------------|--------------|--------------|\n",
    "| LLaMa 7B   | (ml.)g5.4xlarge | 3            | 2048         |\n",
    "| LLaMa 13B  | (ml.)g5.4xlarge | 2            | 2048         |\n",
    "| LLaMa 70B  | (ml.)p4d.24xlarge | 1++ (추가 테스트 필요) | 2048 |\n",
    "\n",
    "- **참고**: g5.2xlarge 대신 g5.4xlarge 인스턴스 유형을 사용할 수도 있지만, LoRA 가중치를 모델 가중치에 병합하려면 모델이 메모리에 맞아야 하므로 merge_weights 매개변수를 사용할 수 없습니다. 그러나 어댑터 가중치를 저장하고 훈련 후 merge_adapter_weights.py를 사용하여 병합할 수 있습니다.\n",
    "\n",
    "- **참고2**: Workshop에는 현재 사용 가능한 인스턴스인 p3.2xlarge 인스턴스를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f237cea3-7e20-433f-8484-87a0e4eaa5e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'epochs': 1,                                      # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                 # batch size for training\n",
    "  'lr': 1e-4,                                       # learning rate used during training\n",
    "  'hf_token': HfFolder.get_token(),                 # huggingface token to access llama 2\n",
    "  'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.p3.2xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "    disable_output_compression = True,         # not compress output to save training time and cost\n",
    "    max_run              = 432000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b80895-cf1e-4ff9-b626-9da75a43eece",
   "metadata": {},
   "source": [
    "이제 .fit() 메서드를 사용하여 우리의 S3 경로를 훈련 스크립트에 전달함으로써 훈련 작업을 시작할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cfee6a-1379-431b-8b73-5e4c7b662e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad60cca1",
   "metadata": {},
   "source": [
    "다음 단계를 실행하기 전 gradio를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d86835-7395-4d78-a90d-5361ab2bf9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio -q"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
